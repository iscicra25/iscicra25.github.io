---
title: "Schedule"
---
**Workshop date and time**: May 19th or 23rd, 2025 in Georgia, Atlanta, USA at [ICRAâ€™25](https://2025.ieee-icra.org).

<style>
tr:first-child {
    max-width: 100px;
}
tr:nth-child(2) {
    max-width: 200px;
}
tr:last-child {
    width: 70%;
}
</style>

<table>
<tbody>
<tr>
<th >
<p><strong>Time</strong></p>
</th>
<th >
<p><strong>Topic</strong></p>
</th>
</tr>
<tr>
<td >
<p>08:45-09:00 AM</p>
</td>
<td>
<p>Registration, welcome, and opening remarks</p>
</td>
</tr>
<tr>
<td >
<p><em>Session I</em></p>
</td>
<td>
<p><em>Capturing &ldquo;Intangible&rdquo; Safety Specifications</em></p>
</td>
</tr>
<tr>
<td >
<p>09:00-09:45 AM</p>
</td>
<td>
<p>Invited talk: <strong>Andrea Bajcsy</strong>, Carnegie Mellon University</p>
<p><strong>Title:</strong> Towards "Open-World" Robot Safety: Unifying Generative AI and Control Systems Safety</p>
<p><strong>Abstract</strong>: Robot safety is a nuanced concept. We commonly equate safety with collision-avoidance, but in complex, real-world environments (i.e., "open world") it can be much more: for example, a safe manipulator should understand when it is not confident about a requested task, and it should hand sharp objects to someone with the sharp edge facing away. Foundational methods in safe control characterize safety constraints as arbitrary sets in state space, and ensure that the robot will always take a best-effort action to avoid entering the safety specification. While in theory this mathematical model is highly expressive, in practice it has been limited to highly controlled robot interactions. The reason why is that, to-date, we have found it challenging to mathematically codify safety in more diverse, nuanced interactions like those that arise in autonomous driving, household, or personal robotics. In this talk, I will provide an overview of my lab's work towards generalizing robot safety by systematically uniting generative AI models and control-theoretic safety techniques.</p>
</td>
</tr>
<tr>
<td >
<p>09:45-10:15 AM</p>
</td>
<td>
<p>Recent PhD graduate talk</p>
</td>
</tr>
<tr>
<td >
<p>10:15-10:45 AM</p>
</td>
<td>
<p>Coffee break with poster session 1</p>
</td>
</tr>
<tr>
<td >
<p>10:45-11:30 AM</p>
</td>
<td>
<p>Invited talk: <strong>Necmiye Ozay</strong>, University of Michigan</p>
<p><strong>Title</strong>: Learning Temporal Logic Constraints from Multi-Modal Human Data</p>
<p><strong>Abstract</strong>: Planning and alignment with human intent, while preserving notions of safety, is crucial for deploying AI-enabled autonomous systems in safety-critical applications. In this talk, I will present our recent work that infuses temporal logic with learning for safety and alignment. In the first part of the talk, I will present a method for learning multi-stage tasks from a small number of demonstrations by learning the logical structure and atomic propositions of a consistent linear temporal logic (LTL) formula. In the second part of the talk, I will show how one can learn to rank different behaviors consistent with a given safety specification from human preferences, while ensuring that rule-violating behaviors are never ranked higher than rule-satisfying ones. These methods will be illustrated with applications in robotics and autonomous driving.</p>
</td>
</tr>
<tr>
<td >
<p><em>Session II</em></p>
</td>
<td>
<p><em>Safe Control of Stochastic and Learning-Enabled Systems</em></p>
</td>
</tr>
<tr>
<td >
<p>11:30-12:15 PM</p>
</td>
<td>
<p>Invited talk: <strong>Luca Laurenti</strong>, TU Delft</p>
<p><strong>Title:</strong> Safety of Stochastic Systems: From Stochastic Barrier Functions to Uncertain Abstractions<br /><strong>Abstract</strong>: Providing safety guarantees for stochastic dynamical systems has become a central problem in many fields, including control theory, machine learning, and robotics. In this talk I will present our recent work on providing safety guarantees for non-linear stochastic dynamical systems, including dynamical systems with neural networks in the loop. I will focus on two different approaches to quantify safety for stochastic systems: Stochastic Barrier Functions (SBFs) and abstractions to uncertain Markov models. While SBFs are analogous to Lyapunov functions to prove (probabilistic) set invariance, abstraction-based approaches approximate the stochastic system into a finite model for the computation of safety probability bounds. I will illustrate pros and cons of both methods. I will then conclude the talk illustrating how recent results from optimal transport and stochastic optimization could be employed to complement both methods to finally provide scalable safety guarantees for non-linear uncertain systems.</p>
</td>
</tr>
<tr>
<td >
<p>12:15-01:30 PM</p>
</td>
<td>
<p>Lunch break</p>
</td>
</tr>
<tr>
<td >
<p>01:30-02:15 PM</p>
</td>
<td>
<p>Invited talk: <strong>Lars Lindemann</strong>, University of Southern California</p>
<p><strong>Title: </strong>Safe Control of Learning-Enabled Autonomous Systems using Conformal Prediction</p>
<p><strong>Abstract: </strong>Learning-enabled autonomous systems promise to enable many future technologies such as autonomous driving, intelligent transportation, and robotics. Accelerated by algorithmic and computational advances in machine learning and AI, there has been tremendous success in the design of learning-enabled autonomous systems. However, these exciting developments are accompanied by new fundamental challenges that arise regarding the safety and reliability of these increasingly complex control systems in which sophisticated algorithms interact with unknown dynamic environments. Imperfect learning and unknowns in the environment require control techniques to rigorously account for such uncertainties. I advocate for the use of conformal prediction (CP) &mdash; a statistical tool for uncertainty quantification &mdash; due to its simplicity, generality, and efficiency as opposed to existing optimization techniques that are either conservative or subject to scalability issues. I first provide an accessible introduction to CP for the non-expert. My goal is then to show how we can use CP to design probabilistically safe motion planning algorithms in dynamic environments. Specifically, we will design a model&nbsp;</p>
</td>
</tr>
<tr>
<td >
<p>02:15-03:00 PM</p>
</td>
<td>
<p>Invited talk: <strong>Bartolomeo Stellato</strong>, Princeton University</p>
<p><strong>Title: </strong>Learning Decision-Focused Uncertainty Sets for Robust Model Predictive Control<br /><strong>Abstract: </strong>We propose a machine learning framework to learn uncertainty sets in robust model predictive control (MPC). Our approach automatically predicts the shape and size of these uncertainty sets based on the current state of the dynamical system. Specifically, we optimize the closed-loop performance while ensuring a desired probability of constraint satisfaction. To achieve this, we employ a stochastic augmented Lagrangian method that differentiates the solutions of robust MPC problems over the key parameters defining the uncertainty sets. Our method is very flexible, capable of learning the parameters of various uncertainty sets frequently employed in practice. We demonstrate the effectiveness of our approach through several numerical examples, achieving significant reductions in both computational complexity and conservatism, while ensuring out-of-sample constraint satisfaction guarantees.</p>
</td>
</tr>
<tr>
<td >
<p><em>Session III</em></p>
</td>
<td>
<p><em>Safety in Robot Perception and Real-World Robot Deployment</em></p>
</td>
</tr>
<tr>
<td >
<p>03:00-03:30 PM</p>
</td>
<td>
<p>Coffee break and Poster session 2</p>
</td>
</tr>
<tr>
<td >
<p>03:30-04:15 PM</p>
</td>
<td>
<p>Invited talk: <strong>Ayoung Kim</strong>, Seoul National University</p>
<p><strong>Title:</strong> Safety in Robot Navigation using Sensors beyond the Visible Spectrum<br /><strong>Abstract: </strong>This talk delves into enhancing long-term robustness in SLAM by focusing on both perception and representation improvements. We will explore how extending sensor capabilities to include radars and thermal cameras can significantly enhance all-day, all-weather perception, which is crucial for maintaining safety in robotic operations, especially in challenging or degraded environments. By integrating these advanced sensors, we aim to address the challenges of operating in varied conditions, ensuring reliable and safe navigation.</p>
</td>
</tr>
<tr>
<td >
<p>04:15-05:00 PM</p>
</td>
<td>
<p>Invited talk: <strong>Georgios Fainekos</strong>, Toyota Motor</p>
<p><strong>Title: </strong>Searching Perception Data Streams For Testing/Training Behaviors <br /><strong>Abstract</strong>: Fortunately, most everyday driving is rather uneventful. Therefore, we can collect an abundance of perception and action data under regular driving conditions for training autonomous vehicles. However, this data is not useful when encountering rare but dangerous situations, such as unusual accidents and near misses. We have developed an abstract language, inspired by regular expressions, to search perception data streams for interesting driving scenarios. These scenarios can form the basis for a data-driven approach to defining and identifying the boundary between safe/desirable and unsafe/undesirable behaviors. To fully automate the process, we demonstrate how perception data can be translated into scenarios for simulation-guided testing or training, and how natural language can be used to query the perception data streams. We conclude by identifying the open challenges and opportunities in searching for interesting driving scenarios and robustifying the autonomy against such scenarios</p>
</td>
</tr>
<tr>
<td >
<p>05:00-05:30 PM</p>
</td>
<td>
<p>Asynchronous Q&amp;A and closing remarks</p>
</td>
</tr>
</tbody>
</table>
