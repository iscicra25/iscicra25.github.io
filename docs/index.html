
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">    
    <meta name="keywords" content="SLAM, perception, deep learning, robots, workshop, ICRA, International Conference on Robotics and Automation">
    <meta name="author" content="Nikolay Atanasov, Luca Carlone">
    <link rel="icon" href="favicon.ico">

    <title>ICRA 2018 Workshop on Representing a Complex World: Perception, Inference, and Learning for Joint Semantic, Geometric, and Physical Understanding</title>

    <!-- Bootstrap core CSS -->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="assets/css/icra18-style.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="assets/js/ie-emulation-modes-warning.js"></script>

    <!-- <script src="assets/js/anchor-with-navbar-fix.js"></script> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    
    <script type="text/javascript">
    <!--
    function toggleAbstract(divid) {
      var x = document.getElementById(divid);
      if (x.style.display === "none") {
          x.style.display = "block";
      } else {
          x.style.display = "none";
      }
    }
    -->
  </script>    
  </head>

  <body>
    <nav class="navbar navbar-inverse navbar-default navbar-fixed-top">
      <div class="container">
      <!--
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Representing a Complex World </br>ICRA 2018 Workshop</a>
        </div>
        -->
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="#overview">Overview</a></li>
            <li><a href="#awards">Call for Papers</a></li>
            <li><a href="#dates">Location</a></li>
            <li><a href="#participation">Discussion</a></li>
            <li><a href="#speakers">Invited Speakers</a></li>
            <li><a href="#schedule">Schedule</a></li>
            <li><a href="#organizers">Committees</a></li>
            <li><a href="#contact">Contact</a></li>
            <!--<li><a href="#support">Travel Support</a></li>-->
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container body">
      <!--
      <div style="height:80px;"></div>
      -->
      <div class="starter-template">
        <img id="topimage" src="assets/fig/icra-mrp18-logo.svg">
      </div>
      <!--
      <div style="height:5px;"></div>
      <div class="starter-template">
        <img id="topimage" name="topimage" src="assets/fig/stata-center-5.4_crp.png">
      </div>
      -->
      
      
      <!------------------ Overview ---------------------------->
      <div class="page-header" id="overview">
        <h1>Overview</h1>
      </div>
        <p> The goal of this workshop is to bring together researchers from robotics, computer vision, machine learning, and neuroscience to examine the challenges and opportunities emerging from the design of environment representations and perception algorithms that unify semantics, geometry, and physics. This goal is motivated by two fundamental observations. First, the development of advanced perception and world understanding is a key requirement for robot autonomy in complex, unstructured environments, and an enabling technology for robot use in transportation, agriculture, mining, construction, security, surveillance, and environmental monitoring. Second, despite the unprecedented progress over the past two decades, there is still a large gap between robot and human perception (e.g., expressiveness of representations, robustness, latency). The workshop aims to bring forward the latest breakthroughs and cutting-edge research on multimodal representations, as well as novel perception, inference, and learning algorithms that can generate such representations. </p>

    <ul>
        <li> The workshop will include <b>keynote presentations</b> from established researchers in robotics, machine learning, computer vision, human and animal perception. </li>
        <li> There will be two <b>panel discussions</b> and two <b>poster sessions</b> highlighting contributed papers throughout the day. </li>
        <li> There will be a demo session including exciting live demos (<b>best demo takes home a monetary prize</b> - see below). </li>
    </ul>
    
    <p> The workshop is endorsed by the <a href="http://www.ieee-ras.org/computer-robot-vision">IEEE RAS Technical Committee for Computer & Robot Vision</a>. </p>
    
    
      <!------------------ Awards ---------------------------->
      <div class="page-header" id="awards">
        <h1>Awards</h1>
      </div>
        <p> To encourage rigorous innovative submissions, this year we plan to award a monetary prize for the best paper and the best demo presented during the workshop. Quality and impact of the submissions will be evaluated by the program committee. The best workshop paper and best demo awards are sponsored by:</p>
        <div class="starter-template">
        <a href="http://isee.ai/"><img id="isee-logo" src="assets/fig/isee-logo.png" style="width:50%"></a>
        </div>

      
      <!------------------ CfP ---------------------------->
      <div class="page-header" id="papers">
        <h1>Call for Papers</h1>
      </div>         
	<p> Participants are invited to submit a <b>full paper</b> (following <a href="http://icra2018.org/call-for-papers">ICRA formatting guidelines</a>) or an <b>extended abstract</b> (up to 2 pages) related to key challenges in unified geometric, semantic, topological, and temporal representations, and associated perception, inference, and learning algorithms. Topics of interest include but are not limited to: </p>

	<ul>
		<li> novel representations that combine geometry, semantics, and physics, and allow reasoning over spatial, semantic, and temporal aspects; </li>
		<li> contextual inference techniques that produce maximum likelihood estimates over hybrid multi-modal representations; </li>
		<li> learning techniques that produce cognitive representations directly from complex sensory inputs; </li>
		<li> approaches that combine learning-based techniques with geometric estimation methods; </li>
		<li> position papers and unconventional ideas on how to reach human-level performance across the broad spectrum of perceptual problems arising in robotics. </li>
	</ul>

  <p> Contributed papers will be reviewed by the organizers and a program committee of invited reviewers. Accepted papers will be published on the workshop website and will be featured in spotlight presentations and poster sessions. We strongly encourage the preparation of live demos to accompany the papers. We plan to select the best submissions and invite the authors of these papers to contribute to a special issue on the <a href="http://www.ieee-ras.org/publications/t-ro">IEEE Transactions on Robotics</a>, related to the topic of the workshop. </p>

	<p> <b>Submission link</b>: <a href="https://easychair.org/conferences/?conf=icramrp18">https://easychair.org/conferences/?conf=icramrp18</a> </p>      
      
      <!------------------ CfP ---------------------------->
      <div class="page-header" id="dates">
        <h1>Important Dates</h1>
      </div>
    <ul>
        <li> <b>Submission Deadline</b>: April 6, 2018. </li>
        <li> <b>Notification of Acceptance</b>: April 30, 2018. </li>
        <li> <b>Workshop Date</b>: May 21, 2018. </li>
        <li> <b>Time</b>: 09:00 - 17:00. </li>
        <li> <b>Room</b>: M1 (Mezzanine Level). </li>
    </ul>

      
      <!------------------ Speakers ---------------------------->
      <div class="page-header" id="participation">
        <h1>Participation</h1>
      </div>
      <p> Feel free to post thought-provoking questions and ideas related to joint metric-semantic-physical perception:
      <ul>
        <li><a href="https://docs.google.com/forms/d/e/1FAIpQLScIe1tJFufwqh-dlNLgXkmMMiJYC_F7kp0sx4JckLTouULhzw/viewform?c=0&w=1">Submit questions here!</a></li>
        <li><a href="https://plus.google.com/communities/102832228492942322585">MultimodalRobotPerception Google Community</a></li>
      </ul>
      Your questions and ideas will be discussed during the panel sessions.</p>
      
      <!--
      We plan to have a broad participation from the robotics community through the <a href="https://plus.google.com/communities/102832228492942322585">MultimodalRobotPerception Google Community</a>. Feel free to post thought-provoking questions and ideas related to joint metric-semantic-physical perception. The posts will be moderated by the organizers and addressed by the invited speakers during the workshop.
      -->



      <div class="page-header" id="speakers">
        <h1>Invited Speakers</h1>
      </div>      
    <ul>
        <li> <a href="https://cs.gmu.edu/~kosecka/">Jana Kosecka</a> (George Mason University) </li>
        <li> <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a> / <a href="https://homes.cs.washington.edu/~barun/">Arun Byravan</a> (University of Washington) </li>
        <li> <a href="https://cs.adelaide.edu.au/~ianr/">Ian Reid</a> (University of Adelaide) </li>
        <li> <a href="https://qbi.uq.edu.au/profile/613/srini-srinivasan/">Srini Srinivasan</a> (Queensland Brain Institute) </li>
        <li> <a href="https://scholar.google.com/citations?user=TDSmCKgAAAAJ">Michael Milford</a> (Queensland University of Technology) </li>
        <li> <a href="http://people.inf.ethz.ch/sattlert/">Torsten Sattler</a> (ETH Zurich) </li>
        <!-- 
        <li> <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a> (University of California, Berkeley) </li>
        <li> <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a> (Imperial College London) </li>
        <li> <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a> (University of California, Berkeley) </li>
        <li> <a href="http://sydney.edu.au/science/psychology/staff/barta/lab/">Bart Anderson</a> (University of Sydney) </li>
        <li> <a href="https://www.inf.ethz.ch/personal/marc.pollefeys/">Marc Pollefeys</a> (ETH Zurich) </li> -->
    </ul>
    
 
 
 
 
    <!-- <a href="presentations/rss17.pdf" target="_blank">[pdf] -->
    <div class="page-header" id="schedule">
        <h1>Schedule</h1>
    </div> 
      <table style="width:98%">
        <tbody>  
          <tr>
            <th> Time </th>
            <th> Topic </th>
          </tr>
            <tr>
                <td> 8:45 - 9:00 </td>
                <td><strong>Registration, welcome, and opening remarks</strong></td>
          </tr>
          <tr>
            <td> 9:00 - 9:30 </td>
            <td> 
            <p><strong>Invited talk:</strong> <a href="https://scholar.google.com/citations?user=TDSmCKgAAAAJ">Michael Milford</a> (Queensland University of Technology)<br/>
            <a href="javascript:toggleAbstract('Michael_Milford')">Adventures in multi-modal, sometimes bio-inspired perception, mapping and navigation for robots and autonomous vehicles</a></p>
            <div id="Michael_Milford" style="display:none">
            <p><strong>Abstract:</strong> I'll take the audience on a whirlwind tour of 15 years of research pushing the boundaries on multi-modal perception and sensing in a mapping and navigation context. I'll cover our forays into biologically inspired sensing and mapping, and touch on some of the opportunities and challenges we faced along the way, including translating the work into applied industrial outcomes. The field is still open for more innovation and we hope the workshop will be a great provocation for discussion and collaboration!</p>
            </div>
            </td>

            
          </tr>
          <tr>
            <td> 9:30 - 10:00 </td>
            <td><p><strong>Invited talk:</strong> <a href="https://cs.gmu.edu/~kosecka/">Jana Kosecka</a> (George Mason University)<br/>
            <a href="javascript:toggleAbstract('Jana_Kosecka')">Semantic Understanding for Robot Perception and Navigation</a></p>
            <div id="Jana_Kosecka" style="display:none">
            <p><strong>Abstract:</strong> Advancements in robotic navigation, mapping, object search and recognition rest to a large extent on robust, efficient and scalable semantic understanding of the surrounding environment. In recent years we have developed several approaches for capturing geometry and semantics of environment from video, RGB-D data, or just simply a single RGB image, focusing on indoors environments relevant for robotics applications. I will demonstrate our work on object detection, semantic segmentation and semantically driven navigation as applicable to find and fetch tasks in indoors environments.</p>
            </div>
            </td>
          </tr>
          <tr>
            <td> 10:00 - 10:30 </td>
            <td><strong>Poster Spotlights</strong><br/>
            <ul>
              <li>A. Gawel, C. Del Don, R. Siegwart, J. Nieto, C. Cadena<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_1.pdf">"X-View: Graph-Based Semantic Multi-View Localization"</a>
              <li>B. Bescos, J. Facil, J. Civera, J. Neira<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_3.pdf">"Detecting, Tracking and Eliminating Dynamic Objects in 3D Mapping using Deep Learning and Inpainting"</a>
              <li>A. Loquercio, D. Scaramuzza<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_4.pdf">"Learning to Control Drones in Natural Environments: A Survey"</a>
              <li>O. Roesler, A. Aly, T. Taniguchi, Y. Hayashi<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_6.pdf">"A Probabilistic Framework for Comparing Syntactic and Semantic Grounding of Synonyms through Cross-Situational Learning"</a>
              <li>A. Milioto, C. Stachniss<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_7.pdf">"Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs"</a>
              <li>F. Nardi, B. Della Corte, G. Grisetti<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_8.pdf">"Unified Representation of Heterogeneous Sets of Geometric Primitives"</a>
              <li>W. Vega-Brown, N. Roy<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_9.pdf">"Admissible abstractions for near-optimal task and motion planning"</a>
              <li>C. Grimm, R. Balasubramanian, M. Sundberg, R. Sherman, A. Kothari, R. Hatton<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_10.pdf">"A Grasping Metric based on Hand-Object Collision"</a>
              <li>S. Daftry, Y. Agrawal, L. Matthies<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_11.pdf">"Online Self-supervised Scene Segmentation for Micro Aerial Vehicles"</a>
              <li>L. Nicholson, M. Milford, N. Suenderhauf<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_14.pdf">"QuadricSLAM: Constrained Dual Quadrics from Object Detections as Landmarks in Semantic SLAM"</a>
              <li>P. Karkus, D. Hsu, W. S. Lee<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_15.pdf">"Particle Filter Networks: End-to-End Probabilistic Localization From Visual Observations"</a>
            </ul>
            </td>
          </tr>
          <tr>
            <td> 10:30 - 11:00 </td>
            <td> <strong>Coffee break</strong> </td>
          </tr>
          <tr>
            <td> 11:00 - 11:30 </td>
            <td><strong>Poster Spotlights</strong><br/>
            <ul>
              <li>M. Henein, G. Kennedy, V. Ila, R. Mahony<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_13.pdf">"Exploiting Rigid Body Motion for SLAM in Dynamic Environments with Applications in Urban Driving and Extrinsic Calibration of a Multi RGBD Camera System"</a>
              <li>J. Park, D. Manocha<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_22.pdf">"Combining Computer Vision and Real Time Motion Planning for Human-Robot Interaction"</a>
              <li>R. Mahjourian, M. Wicke, A. Angelova<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_2.pdf">"Unsupervised Learning of Depth and Ego-Motion from Monocular Video in 3D"</a>
              <li>J. Weibel, T. Patten, M. Vincze<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_16.pdf">"Geometric Priors from Robot Vision in Deep Networks for 3D Object Classification"</a>
              <li>M. Sundermeyer, E. Y. Puang, Z. Marton, M. Durner, R. Triebel<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_17.pdf">"Learning Implicit Representations of 3D Object Orientations from RGB"</a>
              <li>A. Inceoglu, G. Ince, Y. Yaslan, S. Sariel<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_19.pdf">"Comparative Assessment of Sensing Modalities on Manipulation Failure Detection"</a>
              <li>J. Bowkett, J. Burdick, L. Matthies, R. Detry<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_21.pdf">"Semantic Understanding of Task Outcomes: Visually Identifying Failure Modes Autonomously Discovered in Simulation"</a>
              <li>Y. Feldman, V. Indelman<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_23.pdf">"Towards Robust Autonomous Semantic Perception"</a>
              <li>T. Mota, M. Sridharan<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_24.pdf">"Incrementally Grounding Expressions for Spatial Relations between Objects"</a>
              <li>B. X. Chen, R. Sahdev, D. Wu, X. Zhao, M. Papagelis, J. K. Tsotsos<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_25.pdf">"Scene Classification in Indoor Environments for Robots using Context Based Word Embeddings"</a>
              <li>K. Desingh, A. Opipari, O. Jenkins<br/>
                  <a href="assets/ref/ICRA-MRP18_paper_27.pdf">"Analysis of Goal-directed Manipulation in Clutter using Scene Graph Belief Propagation"</a>
            </ul>
            </td>
          </tr>          
          <tr>
            <td> 11:30 - 12:00 </td>
            <td><p><strong>Invited talk:</strong> <a href="https://cs.adelaide.edu.au/~ianr/">Ian Reid</a> (University of Adelaide)<br/>
            <a href="javascript:toggleAbstract('Ian_Reid')">SLAM in the Era of Deep Learning</a></p>
            <div id="Ian_Reid" style="display:none">
            <p><strong>Abstract:</strong> In this talk I will discuss progress in my group over the last few years in moving towards an Object-based system for Localisation and Mapping that maintains the key features and merits of geometric SLAM, but which takes advantage of advances in Deep Learning for detecting objects, performing semantic segmentation, and the ability to regress quantities such as depth for a single view.</p>
            </div>
            </td>
          </tr>
            <tr>
            <td> 12:00 - 12:30 </td>
            <td> <strong>Morning wrap-up: panel discussion</strong> </td>
          </tr>
          <tr>
            <td> 12:30 - 2:00 </td>
            <td> <strong>Lunch break</strong> </td>
          </tr>
          <tr>
            <td> 2:00 - 2:30 </td>
            <td><p><strong>Invited talk:</strong> <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a> / <a href="https://homes.cs.washington.edu/~barun/">Arun Byravan</a> (University of Washington)<br/>
            <a href="javascript:toggleAbstract('Dieter_Fox')">Learning to Predict and Control Objects from Low-level Supervisionn</a></p>
            <div id="Dieter_Fox" style="display:none">
            <p><strong>Abstract:</strong></p>
            </div>
            </td>
          </tr>
          <tr>
            <td> 2:30 - 3:00 </td>
            <td><p><strong>Invited talk:</strong> <a href="https://qbi.uq.edu.au/profile/613/srini-srinivasan/">Srini Srinivasan</a> (Queensland Brain Institute)<br/>
            <a href="javascript:toggleAbstract('Srini_Srinivasan')">Facets of vision, perception, learning and `cognition' in a small brain</a></p>
            <div id="Srini_Srinivasan" style="display:none">
            <p><strong>Abstract:</strong> Honeybees possess a brain about the size of a sesame seed, and yet display surprisingly sophisticated performance in tasks that involve pattern recognition, maze learning, establishing complex associations and learning abstract concepts. This presentation will highlight some of these capacities, and invite speculation on how these tasks might be implemented effectively in what must be relatively simple neural hardware.</p>
            </div>
            </td>
          </tr>
          <tr>
            <td> 3:00 - 3:30 </td>
            <td> <strong>Coffee break & Poster Session</strong> </td>
          </tr>
          <tr>
            <td> 3:30 - 4:00 </td>
            <td> <strong>Poster and demo session</strong> </td>
          </tr>          
          <tr>
            <td> 4:00 - 4:30 </td>
            <td><p><strong>Invited talk:</strong> <a href="http://people.inf.ethz.ch/sattlert/">Torsten Sattler</a> (ETH Zurich)<br/>
            <a href="javascript:toggleAbstract('Torsten_Sattler')">Challenges in Long-Term Visual Localization</a></p>
            <div id="Torsten_Sattler" style="display:none">
            <p><strong>Abstract:</strong> Visual localization is the problem of estimating the position and orientation from which an image was taken with respect to a 3D model of a known scene. This problem has important applications, including autonomous vehicles (including self-driving cars and other robots) and augmented / mixed / virtual reality. While multiple solutions to the visual localization problem exist both in the Robotics and Computer Vision communities for accurate camera pose estimation, they typically assume that the scene does not change over time. However, this assumption is often invalid in practice, both in indoor and outdoor environments. This talk thus briefly discusses the challenges encountered when trying to localize images over a longer period of time. Next, we show how a combination of 3D scene geometry and higher-level scene understanding can help to enable visual localization in conditions where both classical and recently proposed learning-based approaches struggle.</p>
            </div>
            </td>
            
          </tr>
          <tr>
            <td> 4:30 - 5:00 </td>
            <td> <strong>Afternoon wrap-up: panel discussion & closing remarks</strong> </td>
          </tr>    
          <tr>
            <td> 5:00 - 5:15 </td>
            <td> <strong>Award ceremony</strong> </td>
          </tr>             
        </tbody>
      </table>
        <!--
        <p style="margin-left: 2.em;padding: 2em 2em 2em 2em;border-width: 2px;">
            Please complete this 
            <b>
            <a href="https://goo.gl/forms/ixwZXnttlwhSx5Xy2" target="_blank">Participation Form </a>
            </b>!
        </p>
        -->
      <!--
      <div class="page-header" id="posters">
        <h1>Posters</h1>
      </div>
      Title <a href="title.pdf" target="_blank">[pdf]</a></br>
      Author </br></br>
      
      Title <a href="title.pdf" target="_blank">[pdf]</a></br>
      Author </br>                
      -->
      
      
      <!------------ Program Committee ----------->
      <div class="page-header" id="pc">
        <h1>Program Committee</h1>
      </div>
      <ul>
        <li><a href="http://n.ethz.ch/~cesarc">Cesar Cadena</a> (ETHZ)</li>
        <li><a href="http://www.mit.edu/~mrrobot">Kasra Khosoussi</a> (MIT)</li>
        <li><a href="https://fling.seas.upenn.edu/~xiaowz/dynamic/wordpress">Xiaowei Zhou</a> (Zhejiang University)</li>
        <li><a href="http://people.inf.ethz.ch/sattlert/">Torsten Sattler</a> (ETHZ)</li>
        <li><a href="https://homes.cs.washington.edu/~barun">Arunkumar Byravan</a> (UW)</li>
        <li><a href="https://people.eecs.berkeley.edu/~chaene">Christian H&auml;ne</a> (Berkeley)</li>
        <li><a href="http://nikosuenderhauf.info/">Niko S&uuml;nderhauf</a> (QUT)</li>
        <li><a href="http://vindelman.net.technion.ac.il/">Vadim Indelman</a> (Technion)</li>
        <li><a href="http://sites.bu.edu/tron/">Roberto Tron</a> (BU)</li>
        <li><a href="https://karolhausman.github.io/">Karol Hausman</a> (Google Brain)</li>
      </ul>

      <!------------ Organizing Committee ----------->
      <div class="page-header" id="organizers">
        <h1>Organizing Committee</h1>
      </div>
      <ul>
        <li><a href="https://natanaso.github.io/">Nikolay Atanasov</a> &ltnatanasov@ucsd.edu&gt</li>
        <li><a href="https://www.lucacarlone.com/">Luca Carlone</a> &ltlcarlone@mit.edu&gt</li>
      </ul>
      
      <!------------ Contact ----------->
      <div class="page-header" id="contact">
        <h1>Contact</h1>
      </div>
      <p> Should you have any questions, please do not hesitate to contact the organizers Nikolay Atanasov (natanasov@ucsd.edu) or Luca Carlone (lcarlone@mit.edu). Please include ``ICRA 2018 Workshop Submission'' in the subject of the email. </p>
      
      
      <!--
      <div class="page-header" id="support">
        <h1>Travel Support</h1>
      </div>
        Through the generous support of the National Science Foundation, we are happy to announce that the workshop will be able to offer travel mini-awards to <b>U.S.-based workshop contributors</b> to partially cover travel expenses.  All workshop contributors are eligible, and are strongly encouraged to apply for a travel mini-award.  The number of mini-awards to be made available is, however, limited.  Priority will be given to workshop contributors from communities underrepresented in STEM, and/or HBCU/MSI-accredited Universities.  More information will be added soon, please stay tuned!

        <div class="page-header" id="contact">
        <h1>Contact</h1>
      </div>
        Should you have any questions please contact the organizers via <a href="mailto:learning.safe.flight.workshop@gmail.com" target="_top">email.</a>
      -->
        <!--</br></br></br></br> -->
        
        <!--
        <div class="page-header">
            <div style="height:40px;"></div>
            <h3>Workshop supported by:</h3>
        <img id="thanksimage" src="assets/fig/nsf.png">
        </div>
        -->
    </div><!-- /.container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="assets/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
